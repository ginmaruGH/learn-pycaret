{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Functions](https://pycaret.gitbook.io/docs/get-started/functions)\n",
    "\n",
    "## [Optimize（最適化）](https://pycaret.gitbook.io/docs/get-started/functions/optimize)\n",
    "\n",
    "Optimization functions in PyCaret\n",
    "\n",
    "PyCaretの最適化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tune_model（チューンモデル）\n",
    "\n",
    "This function tunes the hyperparameters of the model. The output of this function is a scoring grid with cross-validated scores by fold. The best model is selected based on the metric defined in `optimize` parameter. Metrics evaluated during cross-validation can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function.\n",
    "\n",
    "この関数は、モデルのハイパーパラメータを調整します。この関数の出力は、フォールドごとに交差検証されたスコアを持つスコアリンググリッドです。最適なモデルは、 `optimize` パラメータで定義された指標に基づいて選択されます。交差検証中に評価されたメトリクスは `get_metrics` 関数を用いてアクセスすることができます。カスタムメトリクスは `add_metric` と `remove_metric` 関数で追加・削除することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(data=boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# tune model\n",
    "tuned_dt = tune_model(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_model_default](./images/tune_model_default.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the hyperparameters.\n",
    "\n",
    "ハイパーパラメータを比較する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default model\n",
    "print(dt)\n",
    "\n",
    "# tuned model\n",
    "print(tuned_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_model_default](./images/tune_model_default2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing the iteration（繰り返し回数を増やす）\n",
    "\n",
    "Hyperparameter tuning at the end of the day is an optimization that is constrained by the number of iterations, which eventually depends on how much time and resources you have available. The number of iterations is defined by `n_iter`. By default, it is set to `10`.\n",
    "\n",
    "ハイパーパラメータのチューニングは結局のところ、反復回数に制約される最適化であり、最終的には利用可能な時間とリソースに依存することになります。反復回数は `n_iter` で定義されます。デフォルトでは、`10`に設定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(data=boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# tune model\n",
    "tuned_dt = tune_model(dt, n_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_model_n_iter](./images/tune_model_n_iter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison of 10 and 50 iterations\n",
    "\n",
    "|`n_iter=10`|`n_iter=50`|\n",
    "|:-:|:-:|\n",
    "|![n_iter=10](./images/tune_model_n_iter_10.png)|![n_iter=50](./images/tune_model_n_iter_50.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the metric（メトリクスの選択）\n",
    "\n",
    "When you are tuning the hyperparameters of the model, you must know which metric to optimize for. That can be defined under `optimize` parameter. By default, it is set to `Accuracy` for classification experiments and `R2` for regression.\n",
    "\n",
    "モデルのハイパーパラメータをチューニングする場合、どの指標に対して最適化するか知っておく必要があります。これは `optimize` パラメータで定義することができます。デフォルトでは、分類実験では `Accuracy` に、回帰実験では `R2` に設定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(data=boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# tune model\n",
    "tuned_dt = tune_model(dt, optimize='MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_model_optimize](./images/tune_model_optimize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing custom grid（カスタムグリッドの渡し方）\n",
    "\n",
    "The tuning grid for hyperparameters is already defined by PyCaret for all the models in the library. However, if you wish you can define your own search space by passing a custom grid using `custom_grid` parameter.\n",
    "\n",
    "ハイパーパラメータのチューニンググリッドは、ライブラリ内のすべてのモデルに対して PyCaret によって既に定義されています。しかし、もしあなたが望むなら、 `custom_grid` パラメータを使用してカスタムグリッドを渡すことで、独自の探索空間を定義することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# define search space\n",
    "params = {\n",
    "    \"max_depth\": np.random.randint(1, (len(boston.columns) * 0.85), 20),\n",
    "    \"max_features\": np.random.randint(1, len(boston.columns), 20),\n",
    "    \"min_samples_leaf\": [2, 3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "# tune model\n",
    "tuned_dt = tune_model(dt, custom_grid=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_model_custom_gride](./images/tune_model_custom_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the search algorithm（検索アルゴリズムの変更）\n",
    "\n",
    "PyCaret integrates seamlessly with many different libraries for hyperparameter tuning. This gives you access to many different types of search algorithms including random, bayesian, optuna, TPE, and a few others. All of this just by changing a parameter. By default, PyCaret using `RandomGridSearch` from the sklearn and you can change that by using `search_library` and `search_algorithm` parameter in the `tune_model` function.\n",
    "\n",
    "PyCaret はハイパーパラメータチューニングのための多くの異なるライブラリとシームレスに統合されています。これにより、ランダム、ベイジアン、オプトナ、TPE、その他いくつかのアルゴリズムを含む多くの異なるタイプの検索アルゴリズムにアクセスすることができます。これらはすべて、パラメータを変更するだけで実現できます。デフォルトでは、PyCaret は sklearn の `RandomGridSearch` を使用しますが、 `tune_model` 関数の `search_library` と `search_algorithm` パラメータで変更することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# tune model sklearn\n",
    "tune_model(dt)\n",
    "\n",
    "# tune model optuna\n",
    "tune_model(dt, search_library='optuna')\n",
    "\n",
    "# tune model scikit-optimize\n",
    "tune_model(dt, search_library='scikit-optimize')\n",
    "\n",
    "# tune model tune-sklearn\n",
    "tune_model(dt, search_library ='tune-sklearn', search_algorithm='hyperopt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|||\n",
    "|:-:|:-:|\n",
    "|scikit-learn|optuna|\n",
    "|![tune_mole_search_algo_scikit_learn](./images/tune_model_search_algo_scikit_learn.png)|![tune_mole_search_algo_optuna](./images/tune_model_search_algo_optuna.png)|\n",
    "|scikit-optimize|tune-sklearn|\n",
    "|![tune_mole_search_algo_scikit_optimize](./images/tune_model_search_algo_scikit_optimize.png)|![tune_mole_search_tune_sklearn](./images/tune_model_search_algo_tune_sklearn.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access the tuner（チューナーにアクセスする）\n",
    "\n",
    "By default PyCaret's `tune_model` function only returns the best model as selected by the tuner. Sometimes you may need access to the tuner object as it may contain important attributes, you can use `return_tuner` parameter.\n",
    "\n",
    "PyCaret の `tune_model` 関数は、デフォルトではチューナーによって選択されたベストモデルのみを返します。時には、重要な属性を含むチューナーオブジェクトにアクセスする必要がある場合があるので、 `return_tuner` パラメータを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# tune model and return tuner\n",
    "tuned_model, tuner = tune_model(dt, return_tuner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_model_return_tuner](./images/tune_model_return_tuner.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tuned_model), type(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_model_return_tuner2](./images/tune_model_return_tuner2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_model_return_tuner2](./images/tune_model_return_tuner3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatically choose better（よりよいものを自動的に選択する）\n",
    "\n",
    "Often times the `tune_model` will not improve the model performance. In fact, it may end up making performance worst than the model with default hyperparameters. This may be problematic when you are not actively experimenting in the Notebook rather you have a python script that runs a workflow of `create_model` --> `tune_model` or `compare_models` --> `tune_model`. To overcome this issue, you can use `choose_better`. When set to `True` it will always return a better performing model meaning that if hyperparameter tuning doesn't improve the performance, it will return the input model.\n",
    "\n",
    "多くの場合、`tune_model`はモデルのパフォーマンスを向上させません。実際、デフォルトのハイパーパラメータを用いたモデルよりも性能が悪くなってしまうかもしれません。これは、ノートブックで積極的に実験をするのではなく、 `create_model` --> `tune_model` や `compare_models` --> `tune_model` というワークフローを実行する python スクリプトを持っている場合に問題になるかもしれません。この問題を解決するために、`choose_better` を使用することができます。True` に設定すると、常にパフォーマンスの良いモデルを返します。つまり、ハイパーパラメータのチューニングでパフォーマンスが向上しない場合は、入力モデルを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# tune model\n",
    "dt = tune_model(dt, choose_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tune_model_choose_better](./images/tune_model_choose_better.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: choose_better doesn't affect the scoring grid that is displayed on the screen. The scoring grid will always present the performance of the best model as selected by the tuner, regardless of the fact that output performance < input performance.\n",
    ">\n",
    "> 注： choose_better は画面に表示される採点グリッドに影響を与えません。スコアリング・グリッドは、出力性能 < 入力性能であっても、チューナーによって選択された最良のモデルの性能を常に提示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble_model（アンサンブルモデル）\n",
    "\n",
    "This function ensembles a given estimator. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function.\n",
    "\n",
    "この関数は，与えられた推定量をアンサンブルします。この関数の出力は，畳み込みによるCVスコアを含むスコアリンググリッドです。CV中に評価されたメトリクスは、 `get_metrics` 関数を用いてアクセスすることができます。また、 `add_metric` および `remove_metric` 関数を用いて、カスタムメトリックを追加したり削除したりすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# ensemble model\n",
    "bagged_dt = ensemble_model(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model](./images/ensemble_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bagged_dt)\n",
    "# >>> sklearn.ensemble._bagging.BaggingRegressor\n",
    "\n",
    "print(bagged_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model](./images/ensemble_model2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the fold param（折りたたみパラメーターの変更）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# ensemble model\n",
    "bagged_dt = ensemble_model(dt, fold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_fold](./images/ensemble_model_fold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returned by this is the same as above, however, the performance evaluation is done using 5 fold cross-validation.\n",
    "\n",
    "これによって返されるモデルは上記と同じですが、パフォーマンス評価は 5 分割交差検証を使用して行われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method: Bagging（方法: 袋詰め）\n",
    "\n",
    "Bagging, also known as Bootstrap aggregating, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
    "\n",
    "BaggingはBootstrap aggregatingとも呼ばれ、統計的分類や回帰に用いられる機械学習アルゴリズムの安定性と精度を向上させるために設計された機械学習アンサンブルのメタアルゴリズムです。また、分散を減らし、オーバーフィッティングを回避するのに 役立ちます。通常、決定木法に適用されるが、どのようなタイプの手法にも使用できます。バギングはモデル平均化手法の特殊なケースです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_bagging](./images/ensemble_model_bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method: Boosting（方法: ブースティング）\n",
    "\n",
    "Boosting is an ensemble meta-algorithm for primarily reducing bias and variance in supervised learning. Boosting is in the family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\n",
    "\n",
    "ブースティングは、教師あり学習における偏りや分散を減らすためのアンサンブル・メタ・アルゴリズムです。ブースティングは弱い学習器を強い学習器に変換する機械学習アルゴリズムの一種です。弱い学習器とは、真の分類とわずかな相関しか持たない分類器（ランダムな推測よりもうまく例をラベル付けできる）と定義される。これに対して，強い学習器とは，真の分類と任意によく相関する分類器を指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_boosting](./images/ensemble_model_boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the method（メソッドの選択）\n",
    "\n",
    "There are two possible ways you can ensemble your machine learning model with `ensemble_model`. You can define this in the `method` parameter.\n",
    "\n",
    "`ensemble_model` を用いて機械学習モデルをアンサンブルする方法には、2つの可能性があります。これは `method` パラメータで定義することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# ensemble model\n",
    "boosted_dt = ensemble_model(dt, method='Boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_boosting](./images/ensemble_model_boosting2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(boosted_dt)\n",
    "# >>> sklearn.ensemble._weight_boosting.AdaBoostRegressor\n",
    "\n",
    "print(boosted_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_boosting](./images/ensemble_model_boosting3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing the estimators（推定量を増やす）\n",
    "\n",
    "By default, PyCaret uses 10 estimators for both `Bagging` or `Boosting`. You can increase that by changing `n_estimators` parameter.\n",
    "\n",
    "デフォルトでは、PyCaret は `Bagging` と `Boosting` の両方で 10 個の推定量を使用します。n_estimators` パラメータを変更することで、この値を増やすことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(boston, target='medv')\n",
    "\n",
    "# train model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# ensemble model\n",
    "ensemble_model(dt, n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_n_estimators](./images/ensemble_model_n_estimators.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatically choose better（より良いものを自動的に選択）\n",
    "\n",
    "Often times the `ensemble_model` will not improve the model performance. In fact, it may end up making performance worst than the model with ensembling. This may be problematic when you are not actively experimenting in the Notebook rather you have a python script that runs a workflow of `create_model` --> `ensemble_model` or `compare_models` --> `ensemble_model`. To overcome this issue, you can use `choose_better`. When set to `True` it will always return a better performing model meaning that if hyperparameter tuning doesn't improve the performance, it will return the input model.\n",
    "\n",
    "多くの場合、`ensemble_model`はモデルの性能を向上させることはありません。実際、アンサンブルを行ったモデルよりも性能が悪くなってしまうかもしれません。これは、ノートブックで積極的に実験をするのではなく、 `create_model` --> `ensemble_model` や `compare_models` --> `ensemble_model` というワークフローを実行する Python スクリプトを持っている場合に問題になるかもしれません。この問題を解決するために、`choose_better` を使用することができます。True` に設定すると、常にパフォーマンスの良いモデルを返します。つまり、ハイパーパラメータのチューニングでパフォーマンスが向上しない場合は、入力モデルを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "boston = get_data('boston')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(boston, target='medv')\n",
    "\n",
    "# train model\n",
    "lr = create_model('lr')\n",
    "\n",
    "# ensemble model\n",
    "ensemble_model(lr, choose_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_choose_better](./images/ensemble_model_choose_better.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with `choose_better = True` the model returned from the `ensemble_model` is a simple `LinearRegression` instead of `BaggedRegressor`. This is because the performance of the model didn't improve after ensembling and hence input model is returned.\n",
    "\n",
    "`choose_better = True` の場合、 `ensemble_model` から返されるモデルは `BaggedRegressor` ではなく、単純な `LinearRegression` であることに注意してください。これは、アンサンブルの結果、モデルの性能が向上しなかったため、入力モデルが返されたためです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blend_models（ブレンドモデル）\n",
    "\n",
    "This function trains a Soft Voting / Majority Rule classifier for select models passed in the `estimator_list` parameter. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function.\n",
    "\n",
    "この関数は， `estimator_list` パラメータで渡された選択モデルに対して，ソフト投票/多数決分類器を学習します。この関数の出力は、フォールドごとのCVスコアを含むスコアリンググリッドです。CV中に評価されたメトリクスは `get_metrics` 関数を用いてアクセスすることができます。カスタムメトリクスは `add_metric` および `remove_metric` 関数で追加・削除することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# blend models\n",
    "blender = blend_models([lr, dt, knn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_blend_model](./images/blend_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(blender)\n",
    "# >>> sklearn.ensemble._voting.VotingClassifier\n",
    "\n",
    "print(blender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_blend_model](./images/blend_model2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the fold param（折りたたみパラメーターの変更）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# blend models\n",
    "blender = blend_models([lr, dt, knn], fold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ensemble_model_blend_model](./images/blend_model_fold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returned by this is the same as above, however, the performance evaluation is done using 5 fold cross-validation.\n",
    "\n",
    "これによって返されるモデルは上記と同じですが、パフォーマンス評価は 5 分割交差検証を使用して行われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic input estimators（動的入力推定量）\n",
    "\n",
    "You can also automatically generate the list of input estimators using the [compare_models](./03_02_train.ipynb) function. The benefit of this is that you do not have the change your script at all. Every time the top N models are used as an input list.\n",
    "\n",
    "また，[compare_models](./03_02_train.ipynb) 関数を用いて入力推定量のリストを自動生成することも可能です。この方法の利点は、スクリプトを全く変更する必要がないことです。毎回，上位N個のモデルが入力リストとして使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# blend models\n",
    "blender = blend_models(compare_models(n_select=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blend_model_dynamic_input_estimators](./images/blend_model_dynamic_input_estimators.png)\n",
    "\n",
    "Notice here what happens. We passed `compare_models(n_select = 3)` as an input to `blend_models`. What happened internally is that the `compare_models` function got executed first and the top 3 models are then passed as an input to the `blend_models` function.\n",
    "\n",
    "ここで、何が起こっているかに注目してください。`compare_models(n_select = 3)`を `blend_models` への入力として渡しました。内部的には、まず `compare_models` 関数が実行され、上位 3 つのモデルが `blend_models` 関数の入力として渡されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blend_model_dynamic_input_estimators](./images/blend_model_dynamic_input_estimators2.png)\n",
    "\n",
    "In this example, the top 3 models as evaluated by the `compare_models` are `LogisticRegression`, `LinearDiscriminantAnalysis`, and `RandomForestClassifier`.\n",
    "\n",
    "この例では、 `compare_models` によって評価された上位3つのモデルは、 `LogisticRegression`, `LinearDiscriminantAnalysis`, と `RandomForestClassifier` です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the method（メソッドの変更）\n",
    "\n",
    "When `method = 'soft'`, it predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n",
    "\n",
    "`method = 'soft'` の場合、予測確率の総和のargmaxに基づいてクラスラベルを予測します。これは、よく調整された分類器のアンサンブルに推奨される方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# blend models\n",
    "blender_soft = blend_models([lr,dt,knn], method='soft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blend_model_method](./images/blend_model_method.png)\n",
    "\n",
    "When the `method = 'hard'` , it uses the predictions (hard labels) from input models instead of probabilities.\n",
    "\n",
    "`method = 'hard'` の場合、確率の代わりに入力モデルからの予測値（ハードラベル）を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# blend models\n",
    "blender_hard = blend_models([lr,dt,knn], method='hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blend_model_method](./images/blend_model_method2.png)\n",
    "\n",
    "The default method is set to auto which means it will try to use `soft` method and fall back to `hard` if the former is not supported, this may happen when one of your input models does not support `predict_proba` attribute.\n",
    "\n",
    "デフォルトのメソッドは auto に設定されており、`soft` メソッドを使おうとし、前者がサポートされていない場合は `hard` にフォールバックします。これは、入力モデルのいずれかが `predict_proba` 属性をサポートしていない場合に発生する可能性があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Method parameter is only available in [Classification](https://pycaret.gitbook.io/docs/get-started/modules) module.\n",
    ">\n",
    "> 注：Method パラメータは、[Classification](https://pycaret.gitbook.io/docs/get-started/modules) モジュールでのみ使用できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the weights（ウェイトの変更）\n",
    "\n",
    "By default, all the input models are given equal weight when blending them but you can explicitly pass the weights to be given to each input model.\n",
    "\n",
    "デフォルトでは、すべての入力モデルに同じ重みが与えられてブレンドされるが、各入力モデルに与える重みを明示的に渡すことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# blend models\n",
    "blender_weighted = blend_models([lr,dt,knn], weights=[0.5, 0.2, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blend_model_weights](./images/blend_model_weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also tune the weights of the blender using the `tune_model`.\n",
    "\n",
    "また、`tune_model` を使ってブレンダーの重みを調整することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# blend models\n",
    "blender_weighted = blend_models([lr, dt, knn], weights=[0.5, 0.2, 0.3])\n",
    "\n",
    "# tune blender\n",
    "tuned_blender = tune_model(blender_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blend_model_weights](./images/blend_model_weights2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuned_blender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blend_model_weights](./images/blend_model_weights3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatically choose better（より良いものを自動的に選択）\n",
    "\n",
    "Often times the `blend_models` will not improve the model performance. In fact, it may end up making performance worst than the model with blending. This may be problematic when you are not actively experimenting in the Notebook rather you have a python script that runs a workflow of `compare_models` --> `blend_models`. To overcome this issue, you can use `choose_better`. When set to `True` it will always return a better performing model meaning that if blending the models doesn't improve the performance, it will return the single best performing input model.\n",
    "\n",
    "多くの場合、`blend_models`はモデルのパフォーマンスを向上させることはありません。実際、ブレンドしたモデルよりもパフォーマンスが悪くなってしまうかもしれません。これは、ノートブックで積極的に実験しているのではなく、Pythonスクリプトで `compare_models` --> `blend_models` のワークフローを実行している場合に問題となる可能性があります。この問題を解決するために、 `choose_better` を使用することができます。`True` に設定すると、常にパフォーマンスの良いモデルを返します。つまり、モデルをブレンドしてもパフォーマンスが向上しない場合は、最もパフォーマンスの良い単一の入力モデルを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# blend models\n",
    "blend_models([lr, dt, knn], choose_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blend_model_choose_better](./images/blend_model_choose_better.png)\n",
    "\n",
    "Notice that because `choose_better=True` the final model returned by this function is `LogisticRegression` instead of `VotingClassifier` because the performance of Logistic Regression was most optimized out of all the given input models plus the blender.\n",
    "\n",
    "`choose_better=True` であるため、この関数が返す最終モデルは `VotingClassifier` ではなく `LogisticRegression` であることに注意してください。なぜなら、ロジスティック回帰のパフォーマンスが、与えられたすべての入力モデルとブレンダーの中で最も最適化されていたからです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_models（スタックモデル）\n",
    "\n",
    "This function trains a meta-model over select estimators passed in the `estimator_list` parameter. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function.\n",
    "\n",
    "この関数は、 `estimator_list` パラメータで渡された推定量に対してメタモデルの学習を行います。この関数の出力は、フォールド毎の CV スコアを持つスコアリンググリッドです。CV中に評価されるメトリクスは `get_metrics` 関数で取得することができます。また， `add_metric` および `remove_metric` 関数を用いて、カスタムメトリクスの追加や削除を行うことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# stack models\n",
    "stacker = stack_models([lr, dt, knn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the fold param（折りたたみパラメーターの変更）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# stack models\n",
    "stacker = stack_models([lr, dt, knn], fold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_fold.png)\n",
    "\n",
    "The model returned by this is the same as above, however, the performance evaluation is done using 5 fold cross-validation.\n",
    "\n",
    "これによって返されるモデルは上記と同じですが、パフォーマンス評価は 5 分割交差検証を使用して行われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic input estimators（動的入力推定量）\n",
    "\n",
    "You can also automatically generate the list of input estimators using the [compare_models](./03_02_train.ipynb) function. The benefit of this is that you do not have the change your script at all. Every time the top N models are used as an input list.\n",
    "\n",
    "また，[compare_models](./03_02_train.ipynb)  関数を用いて入力推定量のリストを自動生成することも可能です。この方法の利点は、スクリプトを全く変更する必要がないことです。毎回、上位N個のモデルが入力リストとして使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# stack models\n",
    "stacker = stack_models(compare_models(n_select=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_n_select.png)\n",
    "\n",
    "Notice here what happens. We passed `compare_models(n_select = 3)` as an input to `stack_models`. What happened internally is that the `compare_models` function got executed first and the top 3 models are then passed as an input to the `stack_models` function.\n",
    "\n",
    "ここで、何が起こっているかに注目してください。`compare_models(n_select = 3)`を `stack_models` への入力として渡しました。内部的には、まず `compare_models` 関数が実行され、上位 3 つのモデルが `stack_models` 関数への入力として渡されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_n_select2.png)\n",
    "\n",
    "In this example, the top 3 models as evaluated by the `compare_models` are `LogisticRegression`, `RandomForestClassifier`, and `LGBMClassifier`.\n",
    "\n",
    "この例では、 `compare_models` によって評価された上位3つのモデルは、 `LogisticRegression`, `RandomForestClassifier`, および `LGBMClassifier` であることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the method（メソッドの変更）\n",
    "\n",
    "There are a few different methods you can explicitly choose for stacking or pass `auto` to be automatically determined. When set to `auto`, it will invoke, for each model, `predict_proba`, `decision_function` or `predict` function in that order. Alternatively, you can define the method explicitly.\n",
    "\n",
    "スタッキングにはいくつかの方法があり、明示的に選択するか、`auto`を渡して自動的に決定させることができます。`auto` を指定すると、各モデルに対して、 `predict_proba`, `decision_function`, `predict` の順で関数が呼び出されます。また、明示的にメソッドを定義することも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# stack models\n",
    "stacker = stack_models([lr, dt, knn], method='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the meta-model（メタモデルの変更）\n",
    "\n",
    "When no `meta_model` is passed explicitly, `LogisticRegression` is used for Classification experiments and `LinearRegression` is used for Regression experiments. You can also pass a specific model to be used as a meta-model.\n",
    "\n",
    "明示的に `meta_model` を渡さない場合、分類の実験には `LogisticRegression` が、回帰の実験には `LinearRegression` が利用されます。また、メタモデルとして使用するモデルを指定することも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# train meta-model\n",
    "lightgbm = create_model('lightgbm')\n",
    "\n",
    "# stack models\n",
    "stacker = stack_models([lr, dt, knn], meta_model=lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_meta_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stacker.final_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_meta_model2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restacking（再スタッキング）\n",
    "\n",
    "There are two ways you can stack models. (i) only the predictions of input models will be used as training data for meta-model, (ii) predictions as well as the original training data is used for training meta-model.\n",
    "\n",
    "モデルを積み重ねるには2つの方法があります。(i) 入力モデルの予測値のみをメタモデルの学習データとして使用する方法、(ii) 予測値だけでなく元の学習データもメタモデルの学習データとして使用する方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a few models\n",
    "lr = create_model('lr')\n",
    "dt = create_model('dt')\n",
    "knn = create_model('knn')\n",
    "\n",
    "# stack models\n",
    "stacker = stack_models([lr, dt, knn], restack=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_restack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimize_threshold（最適化閾値）\n",
    "\n",
    "This function optimizes the probability threshold for a trained model. It iterates over performance metrics at different `probability_threshold` with a step size defined in `grid_interval` parameter. This function will display a plot of the performance metrics at each probability threshold and returns the best model based on the metric defined under `optimize` parameter.\n",
    "\n",
    "この関数は、学習済みモデルの確率閾値を最適化します。この関数は、`grid_interval`パラメータで定義されたステップサイズで、異なる `probability_threshold` でパフォーマンスメトリクスを繰り返し処理します。この関数は、各確率閾値におけるパフォーマンスメトリクスのプロットを表示し、 `optimize` パラメータで定義されたメトリックに基づいて最適なモデルを返します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a model\n",
    "knn = create_model('knn')\n",
    "\n",
    "# optimize threshold\n",
    "optimized_knn = optimize_threshold(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_optimize_threshold.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_optimize_threshold2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calibrate_model（キャリブレート・モデル）\n",
    "\n",
    "This function calibrates the probability of a given model using isotonic or logistic regression. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function.\n",
    "\n",
    "この関数は、等張回帰またはロジスティック回帰を用いて、与えられたモデルの確率をキャリブレーションします。この関数の出力は、フォールドごとのCVスコアを持つスコアリンググリッドです。CV中に評価されるメトリクスは、 `get_metrics` 関数を用いてアクセスすることができます。カスタムメトリクスは `add_metric` および `remove_metric` 関数を用いて追加・削除することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train a model\n",
    "dt = create_model('dt')\n",
    "\n",
    "# calibrate model\n",
    "calibrated_dt = calibrate_model(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_calibrate_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calibrated_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack_model](./images/stack_model_calibrate_model2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before and after calibration（キャリブレーション前と後）\n",
    "\n",
    "|||\n",
    "|:-:|:-:|\n",
    "|Before Calibration|After Calibration|\n",
    "|![before calibration](./images/stack_model_before_calibration.png)|![after calibration](./images/stack_model_after_calibration.png)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "144d929da0570c40592d9dc325047f87d05d6a3c34760fe6027613ddef8a9521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
