{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Functions](https://pycaret.gitbook.io/docs/get-started/functions)\n",
    "\n",
    "## [Train（訓練）](https://pycaret.gitbook.io/docs/get-started/functions/train)\n",
    "\n",
    "Training functions in PyCaret\n",
    "\n",
    "PyCaretのトレーニング関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare_models（モデルの比較）\n",
    "\n",
    "This function trains and evaluates the performance of all estimators available in the model library using cross-validation. The output of this function is a scoring grid with average cross-validated scores. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function.\n",
    "\n",
    "この関数は，モデルライブラリで利用可能なすべての推定量を交差検証により訓練し、その性能を評価します。この関数の出力は、交差検証された平均スコアを含むスコアリンググリッドです。CV中に評価されたメトリクスは `get_metrics` 関数を用いてアクセスすることができます。カスタムメトリクスの追加や削除は `add_metric` および `remove_metric` 関数で行うことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# compare models\n",
    "best = compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_default](./images/compare_models_default.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compare_models` returns only the top-performing model based on the criteria defined in `sort` parameter. It is `Accuracy` for classification experiments and `R2` for regression. You can change the `sort` order by passing the name of the metric based on which you want to do model selection.\n",
    "\n",
    "`compare_models` は `sort` パラメータで定義された基準に基づいて、最もパフォーマンスの高いモデルのみを返します。分類の実験では `Accuracy` 、回帰の実験では `R2` となる。モデル選択を行いたい基準の名前を渡すことで、`sort` の順番を変更することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the sort order（ソート順を変更する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# compare models\n",
    "best = compare_models(sort='F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_sort](./images/compare_models_sort.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the sort order of scoring grid is changed now and also the best model returned by this function is selected based on `F1`.\n",
    ">\n",
    "> スコアリンググリッドのソート順が変更され、この関数が返すベストモデルが `F1` に基づいて選択されていることに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_sort2](./images/compare_models_sort2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare only a few models（一部のモデルのみを比較する）\n",
    "\n",
    "If you don't want to do horse racing on the entire model library, you can only compare a few models of your choice by using the `include` parameter.\n",
    "\n",
    "モデルライブラリ全体で競馬をしたくない場合は、`include`パラメータを使うことで、好きなモデル数種類だけを比較することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# compare models\n",
    "best = compare_models(include=['lr', 'dt', 'lightgbm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_include](./images/compare_models_include.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also use `exclude` parameter. This will compare all models except for the ones that are passed in the `exclude` parameter.\n",
    "\n",
    "また、`exclude` パラメータを使用することもできます。これは `exclude` パラメータで渡されたモデル以外のすべてのモデルを比較します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# compare models\n",
    "best = compare_models(exclude=['lr', 'dt', 'lightgbm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_include](./images/compare_models_exclude.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return more than one model（複数のモデルを返す）\n",
    "\n",
    "By default, `compare_models` only return the top-performing model but if you want you can get the Top N models instead of just one model.\n",
    "\n",
    "デフォルトでは `compare_models` はトップパフォーマンスのモデルのみを返しますが、必要であれば、1つのモデルだけでなくトップNのモデルを取得することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# compare models\n",
    "best = compare_models(n_select=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_n_select](./images/compare_models_n_select.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that there is no change in the results, however, if you check the variable `best` , it will now contain a list of the top 3 models instead of just one model as seen previously.\n",
    ">\n",
    "> 結果に変化はありませんが、変数 `best` をチェックすると、以前のように1つのモデルだけでなく、上位3つのモデルのリストが格納されていることに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(best)\n",
    "# >>> list\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_best](./images/compare_models_best.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the budget time（予算時間の設定）\n",
    "\n",
    "If you are running short on time and would like to set a fixed budget time for this function to run, you can do that by setting the `budget_time` parameter.\n",
    "\n",
    "もし、時間がなくて、この関数を実行するための固定予算時間を設定したい場合は、`budget_time`パラメータを設定することによって、それを行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# compare models\n",
    "best = compare_models(budget_time=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_budget_time](./images/compare_models_budget_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the probability threshold（確率の閾値を設定する）\n",
    "\n",
    "When performing binary classification, you can change the probability threshold or cut-off value for hard labels. By default, all classifiers use 0.5 as a default threshold.\n",
    "\n",
    "2値分類を行う際、ハードラベルの確率の閾値またはカットオフ値を変更することができます。デフォルトでは、すべての分類器は0.5をデフォルトの閾値として使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# compare models\n",
    "best = compare_models(probability_threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_probability_threshold](./images/compare_models_probability_threshold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that all metrics except for `AUC` are now different. AUC doesn't change because it's not dependent on the hard labels, everything else is dependent on the hard label which is now obtained using `probability_threshold=0.25`.\n",
    ">\n",
    "> AUC` 以外の全てのメトリクスが異なっていることに注意してください。AUCはハードラベルに依存しないので変わりませんが、それ以外は全て`probability_threshold=0.25`で取得したハードラベルに依存しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: This parameter is only available in the [Classification](https://pycaret.gitbook.io/docs/get-started/modules) module of PyCaret.\n",
    ">\n",
    "> 注：このパラメータは、PyCaret の [Classification](https://pycaret.gitbook.io/docs/get-started/modules) モジュールでのみ使用できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disable cross-validation（交差検証を無効にする）\n",
    "\n",
    "If you don't want to evaluate models using cross-validation and rather just train them and see the metrics on the test/hold-out set you can set the `cross_validation=False`.\n",
    "\n",
    "もし、交差検証を使用してモデルを評価せず、単にモデルをトレーニングしてテスト/ホールドアウト・セットでメトリクスを確認したい場合は、`cross_validation=False`を設定することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# compare models\n",
    "best = compare_models(cross_validation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_cross_validation](./images/compare_models_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks pretty similar but if you focus, the metrics are now different and that's because instead of average cross-validated scores, these are now the metrics on the test/hold-out set.\n",
    "\n",
    "これは、交差検証された平均スコアの代わりに、テスト/ホールドアウトセットのメトリクスが出力されるからです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: This function is only available in [Classification](https://pycaret.gitbook.io/docs/get-started/modules) and [Regression](https://pycaret.gitbook.io/docs/get-started/modules) modules.\n",
    ">\n",
    "> 注：この機能は、[Classification](https://pycaret.gitbook.io/docs/get-started/modules) および [Regression](https://pycaret.gitbook.io/docs/get-started/modules) モジュールでのみ利用可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed training on a cluster（クラスタでの分散学習）\n",
    "\n",
    "To scale on large datasets you can run `compare_models` function on a cluster in distributed mode using a parameter called `parallel`. It leverages the [Fugue](https://github.com/fugue-project/fugue/) abstraction layer to run `compare_models` on Spark or Dask clusters.\n",
    "\n",
    "大規模なデータセットを処理するために、`parallel` というパラメータを使用して分散モードで `compare_models` 関数をクラスタ上で実行することができます。[Fugue](https://github.com/fugue-project/fugue/) の抽象化レイヤーを利用して、Spark や Dask クラスタ上で `compare_models` を実行することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data = diabetes, target = 'Class variable', n_jobs = 1)\n",
    "\n",
    "# create pyspark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# import parallel back-end\n",
    "from pycaret.parallel import FugueBackend\n",
    "\n",
    "# compare models\n",
    "best = compare_models(parallel=FugueBackend(spark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_models_parallel](./images/compare_models_parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that we need to set `n_jobs = 1` in the setup for testing with local Spark because some models will already try to use all available cores, and running such models in parallel can cause deadlocks from resource contention.\n",
    ">\n",
    "> ローカルのSparkでテストする場合は、`n_jobs = 1`とする必要があることに注意してください。\n",
    "そのようなモデルを並列に実行すると、リソースの競合によるデッドロックが発生する可能性があるからです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dask, we can specify the `\"dask\"` inside `FugueBackend` and it will pull the available Dask client.\n",
    "\n",
    "Dask の場合は、`FugueBackend` の内部で `\"dask\"` を指定すると、利用可能な Dask クライアントを引き出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable', n_jobs=1)\n",
    "\n",
    "# import parallel back-end\n",
    "from pycaret.parallel import FugueBackend\n",
    "\n",
    "# compare models\n",
    "best = compare_models(parallel=FugueBackend(\"dask\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the complete example and other features related to distributed execution, check [this example](https://github.com/pycaret/pycaret/blob/master/examples/PyCaret%202%20Fugue%20Integration.ipynb). This example also shows how to get the leaderboard in real-time. In a distributed setting, this involves setting up an RPCClient, but Fugue simplifies that.\n",
    "\n",
    "完全な例と分散実行に関連する他の機能については、[この例](https://github.com/pycaret/pycaret/blob/master/examples/PyCaret%202%20Fugue%20Integration.ipynb)をチェックしてください。この例では、リアルタイムでリーダーボードを取得する方法も示しています。分散環境では、これはRPCClientを設定する必要がありますが、Fugueはそれを簡素化します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_model（モデル作成）\n",
    "\n",
    "This function trains and evaluates the performance of a given estimator using cross-validation. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function. All the available models can be accessed using the `models` function.\n",
    "\n",
    "この関数は，交差検証法を用いて、与えられた推定値の学習と性能の評価を行います。この関数の出力は、フォールド毎の CV スコアを表すスコアリンググリッドです。CV中に評価されたメトリクスには、関数 `get_metrics` を用いてアクセスすることができます。カスタムメトリクスの追加や削除は `add_metric` および `remove_metric` 関数で行うことができます。利用可能なすべてのモデルは `models` 関数を用いてアクセスすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train logistic regression\n",
    "lr = create_model('lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_default](./images/create_model_default.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function displays the performance metrics by fold and the average and standard deviation for each metric and returns the trained model. By default, it uses the `10` fold that can either be changed globally in the [setup](./03_01_initialize.ipynb) function or locally within `create_model`.\n",
    "\n",
    "この関数は、性能指標を fold ごとに表示し、各指標の平均と標準偏差を表示し、学習済みモデルを返します。デフォルトでは `10` 個の fold が使用されるが、これは [setup](./03_01_initialize.ipynb) 関数でグローバルに変更するか、`create_model` 内でローカルに変更することが可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the fold param（折り返しパラメーターの変更）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train logistic regression\n",
    "lr = create_model('lr', fold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_fold_param](./images/create_model_fold_param.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returned by this is the same as above, however, the performance evaluation is done using 5 fold cross-validation.\n",
    "\n",
    "これによって返されるモデルは上記と同じですが、パフォーマンス評価は 5 分割交差検証を使用して行われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model library（モデルライブラリ）\n",
    "\n",
    "To check the list of available models in any module, you can use `models` function.\n",
    "\n",
    "任意のモジュールで利用可能なモデルのリストを確認するには、 `models` 関数を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# check available models\n",
    "models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![models()](./images/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models with custom param（カスタムパラメータを使用したモデル）\n",
    "\n",
    "When you just run `create_model('dt')`, it will train Decision Tree with all default hyperparameter settings. If you would like to change that, simply pass the attributes in the create_model function.\n",
    "\n",
    "`create_model('dt')` を実行すると、デフォルトのハイパーパラメータ設定でデシジョンツリーを学習します。これを変更したい場合は、create_model 関数で属性を指定するだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train decision tree\n",
    "dt = create_model('dt', max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_custom_param](./images/create_model_custom_param.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see models params\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_custom_param](./images/create_model_custom_param2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access the scoring grid（スコアリンググリッドにアクセスする）\n",
    "\n",
    "The performance metrics/scoring grid you see after the `create_model` is only displayed and is not returned. As such, if you want to access that grid as `pandas.DataFrame` you will have to use `pull` command after `create_model`.\n",
    "\n",
    "`create_model` の後に表示されるパフォーマンスメトリクス/スコアリンググリッドは、表示されるだけで、返されることはありません。そのため、もしそのグリッドに `pandas.DataFrame` としてアクセスしたい場合は、 `create_model` の後に `pull` コマンドを使用しなければなりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train decision tree\n",
    "dt = create_model('dt', max_depth=5)\n",
    "\n",
    "# access the scoring grid\n",
    "dt_results = pull()\n",
    "print(dt_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_scoring_grid](./images/create_model_scoring_grid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check type\n",
    "type(dt_results)\n",
    "# >>> pandas.core.frame.DataFrame\n",
    "\n",
    "# select only Mean and SD\n",
    "dt_results.loc[['Mean', 'SD']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_scoring_grid](./images/create_model_scoring_grid2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disable cross-validation（交差検証の無効化）\n",
    "\n",
    "If you don't want to evaluate models using cross-validation and rather just train them and see the metrics on the test/hold-out set you can set the `cross_validation=False`.\n",
    "\n",
    "もし、交差検証を使用してモデルを評価せず、単にモデルをトレーニングしてテスト/ホールドアウト・セットでメトリクスを確認したい場合は、`cross_validation=False`を設定することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train model without cv\n",
    "lr = create_model('lr', cross_validation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_cross_validation](./images/create_model_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the metrics on the test/hold-out set. That's why you only see one row as opposed to the 12 rows in the original output. When you disable `cross_validation`, the model is only trained one time, on the entire training dataset and scored using the test/hold-out set.\n",
    "\n",
    "これらは、テスト/ホールドアウト・セットのメトリクスです。そのため、元の出力では12行あったのが、1行しか表示されないのはそのためです。`cross_validation` を無効にすると、モデルは1回だけ、トレーニングデータセット全体で学習され、テスト/ホールドアウトセットを使って採点されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: This function is only available in [Classification](https://pycaret.gitbook.io/docs/get-started/modules) and [Regression](https://pycaret.gitbook.io/docs/get-started/modules) modules.\n",
    "\n",
    "> 注：この機能は、[Classification](https://pycaret.gitbook.io/docs/get-started/modules) および [Regression](https://pycaret.gitbook.io/docs/get-started/modules) モジュールでのみ利用可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return train score（リターン・トレイン・スコア）\n",
    "\n",
    "The default scoring grid shows the performance metrics on the validation set by fold. If you want to see the performance metrics on the training set by fold as well to examine the over-fitting/under-fitting you can use `return_train_score` parameter.\n",
    "\n",
    "デフォルトのスコアリンググリッドでは、検証用セットのパフォーマンスメトリクスを fold ごとに表示します。もしトレーニングセットのパフォーマンスメトリクスを fold ごとに見たい場合は、 `return_train_score` パラメータを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train model without cv\n",
    "lr = create_model('lr', return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_return_train_score](./images/create_model_return_train_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the probability threshold（確率の閾値を設定する）\n",
    "\n",
    "When performing binary classification, you can change the probability threshold or cut-off value for hard labels. By default, all classifiers use `0.5` as a default threshold.\n",
    "\n",
    "2値分類を行う際、ハードラベルの確率の閾値やカットオフ値を変更することができます。デフォルトでは、すべての分類器は `0.5` をデフォルトの閾値として使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train model with 0.25 threshold\n",
    "lr = create_model('lr', probability_threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_probability_threshold](./images/create_model_probability_threshold.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the model\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_probability_threshold](./images/create_model_probability_threshold2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train models in a loop（ループでモデルをトレーニングする）\n",
    "\n",
    "You can use the `create_model` function in a loop to train multiple models or even the same model with different configurations and compare their results.\n",
    "\n",
    "ループ内で `create_model` 関数を使用すると、複数のモデル、あるいは同じモデルを異なる設定で学習させ、その結果を比較することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# train models in a loop\n",
    "lgbs = [create_model('lightgbm', learning_rate=i) for i in np.arange(0, 1, 0.1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_mode_train_models_in_a_loop](./images/create_model_train_models_in_a_loop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lgbs)\n",
    "# >>> list\n",
    "\n",
    "len(lgbs)\n",
    "# >>> 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to keep track of metrics as well, as in most cases, this is how you can do it.\n",
    "\n",
    "ほとんどの場合そうですが、指標も把握したい場合は、このような方法があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# start a loop\n",
    "models = []\n",
    "results = []\n",
    "\n",
    "for i in np.arange(0.1, 1, 0.1):\n",
    "    model = create_model('lightgbm', learning_rate=i)\n",
    "    model_results = pull().loc[['Mean']]\n",
    "    models.append(model)\n",
    "    results.append(model_results)\n",
    "\n",
    "results = pd.concat(results, axis=0)\n",
    "results.index = np.arange(0.1, 1, 0.1)\n",
    "results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_mode_train_models_in_a_loop](./images/create_model_train_models_in_a_loop2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train custom models（カスタムモデルのトレーニング）\n",
    "\n",
    "You can use your own custom models for training or models from other libraries which are not part of pycaret. As long as their API is consistent with `sklearn`, it will work like a breeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "diabetes = get_data('diabetes')\n",
    "\n",
    "# init setup\n",
    "from pycaret.classification import *\n",
    "clf1 = setup(data=diabetes, target='Class variable')\n",
    "\n",
    "# import custom model\n",
    "from gplearn.genetic import SymbolicClassifier\n",
    "sc = SymbolicClassifier()\n",
    "\n",
    "# train custom model\n",
    "sc_trained = create_model(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_train_custom_models](./images/create_model_train_custom_models.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sc_trained)\n",
    "# >>> gplearn.genetic.SymbolicClassifier\n",
    "\n",
    "print(sc_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![create_model_train_custom_models](./images/create_model_train_custom_models2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your own models（独自のモデルを作成する）\n",
    "\n",
    "You can also write your own class with `fit` and predict function. PyCaret will be compatible with that. Here is a simple example:\n",
    "\n",
    "また、`fit`とpredict関数を使った独自のクラスを書くこともできます。PyCaret はそれと互換性があります。以下は簡単な例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from pycaret.datasets import get_data\n",
    "insurance= get_data('insurance')\n",
    "\n",
    "# init setup\n",
    "from pycaret.regression import *\n",
    "reg1 = setup(data=insurance, target='charges')\n",
    "\n",
    "# create custom estimator\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "class MyOwnModel(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mean = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.mean = y.mean()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array(X.shape[0] * [self.mean])\n",
    "\n",
    "# create an instance\n",
    "my_own_model = MyOwnModel()\n",
    "\n",
    "# train model\n",
    "my_model_trained = create_model(my_own_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![own_model](./images/create_model_own_model.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "144d929da0570c40592d9dc325047f87d05d6a3c34760fe6027613ddef8a9521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
